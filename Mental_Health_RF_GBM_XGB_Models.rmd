---
title: "Mental_Health_Models"
author: "Niket"
date: "July 29, 2018"
output: rmarkdown::github_document
---

```{r}
mhealth_2 <- read.csv(file.choose(), header = TRUE, na.strings = c("NA", "", " ", "."))
```

```{r}
#install.packages('caret')
require(caret)

#install.packages('e1071')
require(e1071)
```

```{r}
mhealth_2$work_interfere <- as.factor(mhealth_2$work_interfere)
table(mhealth_2$work_interfere)
```

```{r}
label_1 <- mhealth_2$work_interfere # our variable for classification
```

```{r}
colnames(mhealth_2)
```


```{r}
set.seed(1234)
oneortwo <- sample(1:2 , length(mhealth_2$Age), replace = TRUE, prob = c(0.8, 0.2)) # generating random values and storing them
```

```{r}
# create train data frame
train_2 <- mhealth_2[oneortwo == 1, -7]

# create test data frame
test_2 <- mhealth_2[oneortwo == 2, -7]

# create data frame to apply train and test upon
train_2_label <- label_1[oneortwo == 1]
test_2_label <- label_1[oneortwo == 2]
```

```{r}
test_2 <- data.frame(test_2, test_2_label)
head(test_2)

train_2 <- data.frame(train_2, train_2_label)
head(train_2)
```

### Random Forest
```{r}
#install.packages('randomForest')
library(randomForest)
```

```{r}
set.seed(1234)
```

```{r}
predicted_rf <- randomForest(train_2_label ~. , data = train_2, importance = TRUE, ntree = 50)
```

We want enough trees to stabilize the error but not so many that they over correlate the ensemble, which will lead to overfit so we keep ntree = 50.

```{r}
varImpPlot(predicted_rf)
```
Higher the value of Gini higher the homogeneity. So split occurs accordingly.

```{r}
plot(predicted_rf, log="y")
```

```{r}
library("party")
```

```{r}
x <- ctree(train_2_label ~. , data = train_2)
plot(x, type = "simple")
```

```{r}
prediction_rf <- predict(predicted_rf, test_2, type = 'class')
results_rf <- data.frame(prediction_rf, test_2$test_2_label)
confusionMatrix(table(results_rf))
```

***

### Generalized Boosted Regression Models(GBM)

```{r}
fitControl <- trainControl(method = "cv", number = 10) #5folds) # cross validation (cv) is used to determine the optimum number of trees. 
```

```{r}
tune_Grid <-  expand.grid(interaction.depth = 2, # interaction.depth = 2, shrinkage = 0.1 came from a bit of experimenting.
                            n.trees = 100,      # n.trees has to be high enough that it is clear the optimum number of trees is lower than the number estimated.
                            shrinkage = 0.1,
                            n.minobsinnode = 20)
```

```{r}
set.seed(1234)

#install.packages('gbm')

predicted_gbm <- train(train_2_label ~. , data = train_2,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE,
                 tuneGrid = tune_Grid)
```


```{r}
prediction_gbm <- predict(predicted_gbm, test_2, type = "raw") 
results_gbm <- data.frame(prediction_gbm, test_2$test_2_label)
confusionMatrix(table(results_gbm))
```


***

### XGBoost

```{r}
# install.packages('xgboost')
library(xgboost)
library(readr)
library(stringr)
```


```{r}
# create train data frame
train_3 <- mhealth_2[oneortwo == 1, -7]

# create test data frame
test_3 <- mhealth_2[oneortwo == 2, -7]

# create data frame to apply train and test upon
train_3_label <- label_1[oneortwo == 1]
test_3_label <- label_1[oneortwo == 2]

# convert every variable to numeric, even the integer variables
train_3 <- as.data.frame(lapply(train_3, as.numeric))
test_3 <- as.data.frame(lapply(test_3, as.numeric))
```

We must convert our data type to numeric, otherwise algorithm doesn’t work.

```{r}
# convert data to xgboost format
data.train_3 <- xgb.DMatrix(data = data.matrix(train_3[, 1:ncol(train_3)]), label = train_3_label)
data.test_3 <- xgb.DMatrix(data = data.matrix(test_3[, 1:ncol(test_3)]), label = test_3_label)
```

```{r}
watchlist <- list(train  = data.train_3, test = data.test_3)
```

```{r}
parameters <- list(
    # General Parameters
    booster            = "gbtree",          # default = "gbtree"           # gbtree (tree based) or gblinear (linear function)
    silent             = 0,                 # default = 0                  # silent = 0 will stop results from displaying
    # Booster Parameters
    eta                = 0.3,               # default = 0.3, range: [0,1]  # Low eta value means model is more robust to overfitting.
    gamma              = 0,                 # default = 0,   range: [0,∞]  # Larger the gamma more conservative the algorithm is.
    max_depth          = 2,                 # default = 6,   range: [1,∞]  # less depth so to avoid overfitting
    min_child_weight   = 1,                 # default = 1,   range: [0,∞]  # It might help in logistic regression when class is extremely imbalanced. 
    subsample          = 1,                 # default = 1,   range: (0,1]  # 0.5 means that XGBoost randomly collected half of the data instances to grow trees, this will prevent overfitting.
    colsample_bytree   = 1,                 # default = 1,   range: (0,1]
    colsample_bylevel  = 1,                 # default = 1,   range: (0,1]
    lambda             = 1,                 # default = 1
    alpha              = 0,                 # default = 0
    # Task Parameters
    objective          = "multi:softmax",   # default = "reg:linear"
    eval_metric        = "mlogloss",
    num_class          = 20,
    seed               = 1234               # reproducability seed
    )
```

```{r}
predicted_xgb <- xgb.train(parameters, data.train_3, nrounds = 100, watchlist) # nrounds is like ntrees 
```

```{r}
prediction_xgb <- predict(predicted_xgb, data.test_3)
table(prediction_xgb)
```
Values have to be changed to 0 1 2 and 3
```{r}
prediction_xgb[prediction_xgb == 1] <- 0
prediction_xgb[prediction_xgb == 2] <- 1
prediction_xgb[prediction_xgb == 3] <- 2
prediction_xgb[prediction_xgb == 4] <- 3
```

```{r}
results_xgb <- data.frame(prediction_xgb, test_3_label)
confusionMatrix(table(results_xgb))
```

***

All the models are nearly 50% accurate

